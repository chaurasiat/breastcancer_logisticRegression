{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaurasiat/breastcancer_logisticRegression/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "ERyD7EKgBjFP",
        "colab_type": "text"
      },
      "source": [
        "<center> <h1> CSE 574 Project 1 </h1> </center>\n",
        "\n",
        "<center> <h2> Authors: Mihir Chauhan, Sargur Srihari </h2> </center>\n",
        "\n",
        "<center> <h2> Due Time and Date: 11:59PM October 7th 2020 </h2> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQpRVkbIBjFQ",
        "colab_type": "text"
      },
      "source": [
        "### Project 1 Task\n",
        "\n",
        "The task of this project is to perform classification using machine learning for a two class problem. The features used for classification are pre-computed from images of a fine needle aspirate (FNA) ofa breast mass.  Your task is to classify suspected FNA cells to Benign (class 0) or Malignant (class 1) using logistic regression as the classifier. The dataset in use is the Wisconsin Diagnostic Breast Cancer (wdbc.csv).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jT41D0BBjFR",
        "colab_type": "text"
      },
      "source": [
        "### Dataset Description\n",
        "\n",
        "You will be using Wisconsin Diagnostic Breast Cancer (WDBC) dataset for training, validation and testing. The  dataset you are provided with is wdbc.csv  which contains  500  data points with  31  attributes (diagnosis  (B/M),  30  real-valued  inputfeatures). \n",
        "\n",
        "####  How are the 30 features computed? (Below info. is just for your knowledge)\n",
        "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breastmass.  Computed features describes the following characteristics of the cell nuclei present in the image. \n",
        "\n",
        "|    |                              Feature                              |\n",
        "|----|:-----------------------------------------------------------------:|\n",
        "| 1  | radius (mean of distances from center to points on the perimeter) |\n",
        "| 2  | texture (standard deviation of gray-scale                         |\n",
        "| 3  | perimeter                                                         |\n",
        "| 4  | area                                                              |\n",
        "| 5  | smoothness (local variation in radius lengths)                    |\n",
        "| 6  | compactness (perimeter2/area − 1.0)                               |\n",
        "| 7  | concavity (severity of concave portions of the contour)           |\n",
        "| 8  | concave points (number of concave portions of the contour)        |\n",
        "| 9  | symmetry                                                          |\n",
        "| 10 | fractal dimension (“coastline approximation” - 1)                 |\n",
        "\n",
        "The mean, standard error, and “worst” or largest (mean of the three largest values) of these features were computed for each image, resulting in <b>30 features<b>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ckmZo8lBjFS",
        "colab_type": "text"
      },
      "source": [
        "### Plan of work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVEiY_CgBjFT",
        "colab_type": "text"
      },
      "source": [
        "#### STEP 1: Import Libraries\n",
        "\n",
        "You are NOT ALLOWED to use any libraries for directly implementing Logistic Regression.\n",
        "\n",
        "For eg. [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) <font color='red'>NOT ALLLOWED</font>\n",
        "\n",
        "You need to implement Logistic Regression from scratch using Gradient Descent Optimization Algorithm. \n",
        "\n",
        "You can use libraries for:\n",
        "* loading data (Pandas, Numpy), <font color='green'>ALLLOWED</font>\n",
        "* Preprocessing Data (sklearn > preprocessing), <font color='green'>ALLLOWED</font>\n",
        "* Partitioning Data (sklearns > train_test_split), <font color='green'>ALLLOWED</font>\n",
        "* Plotting Graphs (Matplotlib) <font color='green'>ALLLOWED</font>\n",
        "* Finding Accuracy, Precision, Recall using sklearn.metrics <font color='green'>ALLLOWED</font>\n",
        "\n",
        "You can alternatively use other libraries to implement any sub-task (e.g. loading, partitioning etc.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3HEudUXBjFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import savetxt, loadtxt\n",
        "% matplotlib inline"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxWDqEHcBjFY",
        "colab_type": "text"
      },
      "source": [
        "#### Step 2: Data Loading <font color='blue'>(5 Points)</font>\n",
        "\n",
        "1. Read 'wdbc.csv' data using Pandas library. Load the data in a dataframe\n",
        "2. Drop first row as it is the header row\n",
        "2. Map Malignant to Class 1 and Benign to Class 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-6nOqHqBjFY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "9404c59d-0ebc-4533-a8d5-8a773172c71f"
      },
      "source": [
        "data = pd.read_csv(\"wdbc.csv\") \n",
        "data\n",
        "X = data.loc[:, data.columns != 'y'] \n",
        "Y = data['y'] \n",
        "X,Y\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(         x1     x2      x3      x4  ...      x27      x28     x29      x30\n",
              " 0    17.990  10.38  122.80  1001.0  ...  0.71190  0.26540  0.4601  0.11890\n",
              " 1    20.570  17.77  132.90  1326.0  ...  0.24160  0.18600  0.2750  0.08902\n",
              " 2    19.690  21.25  130.00  1203.0  ...  0.45040  0.24300  0.3613  0.08758\n",
              " 3    11.420  20.38   77.58   386.1  ...  0.68690  0.25750  0.6638  0.17300\n",
              " 4    20.290  14.34  135.10  1297.0  ...  0.40000  0.16250  0.2364  0.07678\n",
              " ..      ...    ...     ...     ...  ...      ...      ...     ...      ...\n",
              " 495  10.290  27.61   65.67   321.4  ...  0.20000  0.09127  0.2226  0.08283\n",
              " 496  10.160  19.59   64.73   311.7  ...  0.01005  0.02232  0.2262  0.06742\n",
              " 497   9.423  27.88   59.26   271.3  ...  0.00000  0.00000  0.2475  0.06969\n",
              " 498  14.590  22.68   96.39   657.1  ...  0.36620  0.11050  0.2258  0.08004\n",
              " 499  11.510  23.93   74.52   403.5  ...  0.36300  0.09653  0.2112  0.08732\n",
              " \n",
              " [500 rows x 30 columns], 0      M\n",
              " 1      M\n",
              " 2      M\n",
              " 3      M\n",
              " 4      M\n",
              "       ..\n",
              " 495    B\n",
              " 496    B\n",
              " 497    B\n",
              " 498    B\n",
              " 499    B\n",
              " Name: y, Length: 500, dtype: object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbeHTnQyBjFb",
        "colab_type": "text"
      },
      "source": [
        "#### Step 3: Data Partitioning <font color='blue'>(5 Points)</font>\n",
        "\n",
        "1. Partition your data into training (80%), validation (20%) and testing data(20%) using sklearn library (Hint: use train_test_split)\n",
        "2. Seperate Target Label (y) and Features (x1 to x30) for training, validation and testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm8hCEB7BjFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWJ5cSu5BjFg",
        "colab_type": "text"
      },
      "source": [
        "#### Step 4: Scaling Features <font color='blue'>(5 Points)</font>\n",
        "\n",
        "One simple scaling function that you could use off the shelf is Min Max Scaler function of Sklearns. Min Max scaler function transforms features by scaling each feature to a range between <b> 0 and 1 </b>. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n",
        "\n",
        "The transformation is given by:\n",
        "\n",
        "$X_{std} = \\frac{X - X_{min}}{X_{max} - X_{min}}$\n",
        "\n",
        "$X_{scaled} = X_{std} * (maxRange - minRange) + minRange$\n",
        "\n",
        "$maxRange$ = 1, <br>\n",
        "$minRange$ = 0, <br>\n",
        "$X_{max}$ and $X_{min}$ are over axis = 0 (Each columns max and min value) \n",
        "\n",
        "##### Why do we need to scale features?\n",
        "We scale the data to bring all the features to the same range (in our case between 0 and 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdirt44JBjFh",
        "colab_type": "text"
      },
      "source": [
        "#### Hint, make sure the dimensionality of the training features, weights, biases are consistent for np.dot and np.multiply"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFryF8NXBjFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvMupdggBjFm",
        "colab_type": "text"
      },
      "source": [
        "#### Step 5: Initialization of Variables\n",
        "* Initialize Hyper-Parameters (Learning Rate, Number of Epochs) to some value\n",
        "* Initialize weights to any random values (We have initialized weights as an array random values sampled from a normal distribution)\n",
        "* Initialize bias to any scalar value (We have initialized bias to value 0)\n",
        "* Initialize other variables which may be used for tracking cost, number of data points etc.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIAIAyCTBjFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper parameter initialization\n",
        "learningrate = 1\n",
        "epochs = 1\n",
        "bias = 0\n",
        "\n",
        "# intialize matrix with random values for weights with normal distribution\n",
        "# Return a sample (or samples) from the “standard normal” distribution.\n",
        "weights = np.random.randn(30,1)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkaBAQ4RBjFr",
        "colab_type": "text"
      },
      "source": [
        "#### Step 6: TRAINING with Logistic Regression Implementation using Gradient Descent Algorithm <font color='blue'>(30 Points)</font>\n",
        "\n",
        "Iteratively update the weights and biases for each epoch using:\n",
        "* Step 6.1: Use genesis equation $\\hat{y} = \\sigma (W.X + b)$ where $W$ is the weight array, $X$ is the input features and $\\hat{y}$ is the predicted value which will be between 0 and 1. (You will have to perform same operation on validation set as well)\n",
        "* Step 6.2: Find Binary Cross Entropy Cost for training and validation set using predicted value $\\hat{y}$ and truth value $y$\n",
        "* Step 6.3: Find $ \\Delta W = \\frac{\\delta L}{\\delta W}$ and $ \\Delta b = \\frac{\\delta L}{\\delta b}$ (Proof for finding  $ \\Delta W$ and $\\Delta b$ is available in prof. slides)\n",
        "* Step 6.4: Update $W$ and $b$ using learning rate as follows:\n",
        "  - $W = W - learningRate*\\Delta W$\n",
        "  - $b = b - learningRate*\\Delta b$\n",
        "* Step 6.5: Store BCE Cost for training and validation in seperate cost tracking list\n",
        "* Step 6.6: Calculate Training and Validation Accuracy and store in seperate accuracy tracking list (<b>Hint</b>: Threshold $\\hat{y}$ to 0.5 for category determination for finding accuracy)\n",
        "\n",
        "Run step 6 multiple times, each time with a different set of hyperparameters and determine the best set of hyperparameters which gives best training and validation accuracy. \n",
        "\n",
        "Corresponding to the best set of hyper parameters, you should get the best set of weights and bias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIwHmVA0BjFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For each epoch:\n",
        "    \n",
        "    # Step 6.1\n",
        "    \n",
        "    # Step 6.2\n",
        "    \n",
        "    # Step 6.3\n",
        "    \n",
        "    # Step 6.4\n",
        "    \n",
        "    # Step 6.5\n",
        "    \n",
        "    # Step 6.6"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-ZcnoTOBjFv",
        "colab_type": "text"
      },
      "source": [
        "After performing all the steps above, the best set of updated weights and bias should be stored as a <i>weights_biases.csv</i> file. Your <i>weights_biases.csv</i> will tested on a hidden test set and you would be graded on how well your model (weights) performed on this hidden set <font color='blue'>(30 Points)</font>\n",
        "\n",
        "<b>(Do not change the code provided in the cell below for storing the weights and bias)</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg8pxLjcBjFw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56f16c42-4145-4a17-8c89-9c22c549db96"
      },
      "source": [
        "# Save the weights file (DO NOT CHANGE THIS CODE)\n",
        "weights_bias = np.append(weights,bias)\n",
        "\n",
        "if weights_bias.shape == (31,):\n",
        "    print(\"Weights and Bias consistent :) \")\n",
        "    savetxt('weights_bias.csv', weights_bias, delimiter=',')\n",
        "else:\n",
        "    print(\"Weights and Bias inconsistent :( \\\\\n",
        "          Weights array shape should be (30,) and Bias should be a scalar \\\\\n",
        "          weights_bias variable should be shaped as (31,)\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights and Bias consistent :) \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l_NEBUaBjFz",
        "colab_type": "text"
      },
      "source": [
        "#### Step 7: Plot Training and Validation Cost vs Number of Epochs <font color='blue'>(5 Points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "JDi6_DhUBjF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2RFOL_7BjF2",
        "colab_type": "text"
      },
      "source": [
        "####  Step 8: Plot Training and Validation Accuracy vs Number of Epochs <font color='blue'>(5 Points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkYXaH9uBjF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJaGCDDiBjF6",
        "colab_type": "text"
      },
      "source": [
        "#### Step 9: Test your model using tesing data <font color='blue'>(15 Points)</font>\n",
        "\n",
        "* Step 9.1: Use genesis equation $\\hat{y} = \\sigma (W.X_{test} + b)$ where $W$ is the weight array, $X_{test}$ is the input test features and $\\hat{y}$ is the predicted value which will be between 0 and 1.\n",
        "* Step 9.2: Threshold $\\hat{y}$ at 0.5 to find the category for each data point.\n",
        "* Step 9.3: Find accuracy, precision and recall for testing data (you can use sklearns.metrics library)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LowhLQneBjF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_81gvknWBjF-",
        "colab_type": "text"
      },
      "source": [
        "#### Step 10: Submission to timberlake server\n",
        "\n",
        "* The code for your implementation should be in this Python notebook with necessary comments within the code.\n",
        "\n",
        "* Your <b> Python Code file </b> `main.ipynb`, <b> Data File </b> `wdbc.csv` and your <b>Trained Weights and Bias File</b> `weights_bias.csv`</b> should be put in a single folder named as `proj1code`. \n",
        "\n",
        "* `proj1code` folder should be zipped with the resulting zip file name as `proj1code.zip`.\n",
        "\n",
        "* Submit the Python code on CSE timberlake server with the following script:\n",
        "\n",
        " - `submit_cse474 proj1code.zip` for undergraduates\n",
        " - `submit_cse574 proj1code.zip` for graduates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9AL05GZBjF_",
        "colab_type": "text"
      },
      "source": [
        "### Grading Rubric\n",
        "* <b>30 Points:</b> Your trained `weights_bias.csv` will be automatically graded using a script on unbiased hidden test data file. Hence, it is important that your `weights_biases` should of dimensionality (31,) and properly trained.\n",
        "* <b>30 Points:</b> Training logic for implementing logistic regression (Step 6)\n",
        "* <b>15 Points:</b> Testing Accuracy, Precision and Recall (Step 9)\n",
        "* <b>5 Points:</b> Plot of Training and Validation cost vs No. of epochs (Step 7) \n",
        "* <b>5 Points:</b> Plot of Training and Validation accuracy vs No. of epochs (Step 8)\n",
        "* <b> 5 points: </b> Scaling features (Step 4)\n",
        "* <b> 5 points: </b> Partitioning Data (Step 3)\n",
        "* <b> 5 points: </b> Data loading (Step 2)"
      ]
    }
  ]
}