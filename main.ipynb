{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "ERyD7EKgBjFP"
      },
      "source": [
        "<center> <h1> CSE 574 Project 1 </h1> </center>\n",
        "\n",
        "<center> <h2> Authors: Mihir Chauhan, Sargur Srihari </h2> </center>\n",
        "\n",
        "<center> <h2> Due Time and Date: 11:59PM October 7th 2020 </h2> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQpRVkbIBjFQ"
      },
      "source": [
        "### Project 1 Task\n",
        "\n",
        "The task of this project is to perform classification using machine learning for a two class problem. The features used for classification are pre-computed from images of a fine needle aspirate (FNA) ofa breast mass.  Your task is to classify suspected FNA cells to Benign (class 0) or Malignant (class 1) using logistic regression as the classifier. The dataset in use is the Wisconsin Diagnostic Breast Cancer (wdbc.csv).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jT41D0BBjFR"
      },
      "source": [
        "### Dataset Description\n",
        "\n",
        "You will be using Wisconsin Diagnostic Breast Cancer (WDBC) dataset for training, validation and testing. The  dataset you are provided with is wdbc.csv  which contains  500  data points with  31  attributes (diagnosis  (B/M),  30  real-valued  inputfeatures). \n",
        "\n",
        "####  How are the 30 features computed? (Below info. is just for your knowledge)\n",
        "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breastmass.  Computed features describes the following characteristics of the cell nuclei present in the image. \n",
        "\n",
        "|    |                              Feature                              |\n",
        "|----|:-----------------------------------------------------------------:|\n",
        "| 1  | radius (mean of distances from center to points on the perimeter) |\n",
        "| 2  | texture (standard deviation of gray-scale                         |\n",
        "| 3  | perimeter                                                         |\n",
        "| 4  | area                                                              |\n",
        "| 5  | smoothness (local variation in radius lengths)                    |\n",
        "| 6  | compactness (perimeter2/area − 1.0)                               |\n",
        "| 7  | concavity (severity of concave portions of the contour)           |\n",
        "| 8  | concave points (number of concave portions of the contour)        |\n",
        "| 9  | symmetry                                                          |\n",
        "| 10 | fractal dimension (“coastline approximation” - 1)                 |\n",
        "\n",
        "The mean, standard error, and “worst” or largest (mean of the three largest values) of these features were computed for each image, resulting in <b>30 features<b>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ckmZo8lBjFS"
      },
      "source": [
        "### Plan of work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVEiY_CgBjFT"
      },
      "source": [
        "#### STEP 1: Import Libraries\n",
        "\n",
        "You are NOT ALLOWED to use any libraries for directly implementing Logistic Regression.\n",
        "\n",
        "For eg. [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) <font color='red'>NOT ALLLOWED</font>\n",
        "\n",
        "You need to implement Logistic Regression from scratch using Gradient Descent Optimization Algorithm. \n",
        "\n",
        "You can use libraries for:\n",
        "* loading data (Pandas, Numpy), <font color='green'>ALLLOWED</font>\n",
        "* Preprocessing Data (sklearn > preprocessing), <font color='green'>ALLLOWED</font>\n",
        "* Partitioning Data (sklearns > train_test_split), <font color='green'>ALLLOWED</font>\n",
        "* Plotting Graphs (Matplotlib) <font color='green'>ALLLOWED</font>\n",
        "* Finding Accuracy, Precision, Recall using sklearn.metrics <font color='green'>ALLLOWED</font>\n",
        "\n",
        "You can alternatively use other libraries to implement any sub-task (e.g. loading, partitioning etc.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3HEudUXBjFT"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import savetxt, loadtxt\n",
        "% matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxWDqEHcBjFY"
      },
      "source": [
        "#### Step 2: Data Loading <font color='blue'>(5 Points)</font>\n",
        "\n",
        "1. Read 'wdbc.csv' data using Pandas library. Load the data in a dataframe\n",
        "2. Drop first row as it is the header row\n",
        "2. Map Malignant to Class 1 and Benign to Class 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-6nOqHqBjFY",
        "outputId": "2753bb87-761c-4b18-db42-e223434648ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "source": [
        "data = pd.read_csv(\"wdbc.csv\") \n",
        "data.shape\n",
        "data.head(10)\n",
        "# data.isna().sum()\n",
        "\n",
        "# X = data.iloc[:, data.columns != 'y'].values\n",
        "# # Y = data['y'] \n",
        "# # X,Y\n",
        "# Y = Y.map({'M': 1, 'B': 0})\n",
        "# Y=Y.values\n",
        "# data.shape\n",
        "# data.isna().sum()\n",
        "# data['y'].value_counts()\n",
        "# data.dtypes\n",
        "# type(X,Y.values)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>x3</th>\n",
              "      <th>x4</th>\n",
              "      <th>x5</th>\n",
              "      <th>x6</th>\n",
              "      <th>x7</th>\n",
              "      <th>x8</th>\n",
              "      <th>x9</th>\n",
              "      <th>x10</th>\n",
              "      <th>x11</th>\n",
              "      <th>x12</th>\n",
              "      <th>x13</th>\n",
              "      <th>x14</th>\n",
              "      <th>x15</th>\n",
              "      <th>x16</th>\n",
              "      <th>x17</th>\n",
              "      <th>x18</th>\n",
              "      <th>x19</th>\n",
              "      <th>x20</th>\n",
              "      <th>x21</th>\n",
              "      <th>x22</th>\n",
              "      <th>x23</th>\n",
              "      <th>x24</th>\n",
              "      <th>x25</th>\n",
              "      <th>x26</th>\n",
              "      <th>x27</th>\n",
              "      <th>x28</th>\n",
              "      <th>x29</th>\n",
              "      <th>x30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>M</td>\n",
              "      <td>12.45</td>\n",
              "      <td>15.70</td>\n",
              "      <td>82.57</td>\n",
              "      <td>477.1</td>\n",
              "      <td>0.12780</td>\n",
              "      <td>0.17000</td>\n",
              "      <td>0.15780</td>\n",
              "      <td>0.08089</td>\n",
              "      <td>0.2087</td>\n",
              "      <td>0.07613</td>\n",
              "      <td>0.3345</td>\n",
              "      <td>0.8902</td>\n",
              "      <td>2.217</td>\n",
              "      <td>27.19</td>\n",
              "      <td>0.007510</td>\n",
              "      <td>0.03345</td>\n",
              "      <td>0.03672</td>\n",
              "      <td>0.01137</td>\n",
              "      <td>0.02165</td>\n",
              "      <td>0.005082</td>\n",
              "      <td>15.47</td>\n",
              "      <td>23.75</td>\n",
              "      <td>103.40</td>\n",
              "      <td>741.6</td>\n",
              "      <td>0.1791</td>\n",
              "      <td>0.5249</td>\n",
              "      <td>0.5355</td>\n",
              "      <td>0.1741</td>\n",
              "      <td>0.3985</td>\n",
              "      <td>0.12440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>M</td>\n",
              "      <td>18.25</td>\n",
              "      <td>19.98</td>\n",
              "      <td>119.60</td>\n",
              "      <td>1040.0</td>\n",
              "      <td>0.09463</td>\n",
              "      <td>0.10900</td>\n",
              "      <td>0.11270</td>\n",
              "      <td>0.07400</td>\n",
              "      <td>0.1794</td>\n",
              "      <td>0.05742</td>\n",
              "      <td>0.4467</td>\n",
              "      <td>0.7732</td>\n",
              "      <td>3.180</td>\n",
              "      <td>53.91</td>\n",
              "      <td>0.004314</td>\n",
              "      <td>0.01382</td>\n",
              "      <td>0.02254</td>\n",
              "      <td>0.01039</td>\n",
              "      <td>0.01369</td>\n",
              "      <td>0.002179</td>\n",
              "      <td>22.88</td>\n",
              "      <td>27.66</td>\n",
              "      <td>153.20</td>\n",
              "      <td>1606.0</td>\n",
              "      <td>0.1442</td>\n",
              "      <td>0.2576</td>\n",
              "      <td>0.3784</td>\n",
              "      <td>0.1932</td>\n",
              "      <td>0.3063</td>\n",
              "      <td>0.08368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>M</td>\n",
              "      <td>13.71</td>\n",
              "      <td>20.83</td>\n",
              "      <td>90.20</td>\n",
              "      <td>577.9</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>0.16450</td>\n",
              "      <td>0.09366</td>\n",
              "      <td>0.05985</td>\n",
              "      <td>0.2196</td>\n",
              "      <td>0.07451</td>\n",
              "      <td>0.5835</td>\n",
              "      <td>1.3770</td>\n",
              "      <td>3.856</td>\n",
              "      <td>50.96</td>\n",
              "      <td>0.008805</td>\n",
              "      <td>0.03029</td>\n",
              "      <td>0.02488</td>\n",
              "      <td>0.01448</td>\n",
              "      <td>0.01486</td>\n",
              "      <td>0.005412</td>\n",
              "      <td>17.06</td>\n",
              "      <td>28.14</td>\n",
              "      <td>110.60</td>\n",
              "      <td>897.0</td>\n",
              "      <td>0.1654</td>\n",
              "      <td>0.3682</td>\n",
              "      <td>0.2678</td>\n",
              "      <td>0.1556</td>\n",
              "      <td>0.3196</td>\n",
              "      <td>0.11510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>M</td>\n",
              "      <td>13.00</td>\n",
              "      <td>21.82</td>\n",
              "      <td>87.50</td>\n",
              "      <td>519.8</td>\n",
              "      <td>0.12730</td>\n",
              "      <td>0.19320</td>\n",
              "      <td>0.18590</td>\n",
              "      <td>0.09353</td>\n",
              "      <td>0.2350</td>\n",
              "      <td>0.07389</td>\n",
              "      <td>0.3063</td>\n",
              "      <td>1.0020</td>\n",
              "      <td>2.406</td>\n",
              "      <td>24.32</td>\n",
              "      <td>0.005731</td>\n",
              "      <td>0.03502</td>\n",
              "      <td>0.03553</td>\n",
              "      <td>0.01226</td>\n",
              "      <td>0.02143</td>\n",
              "      <td>0.003749</td>\n",
              "      <td>15.49</td>\n",
              "      <td>30.73</td>\n",
              "      <td>106.20</td>\n",
              "      <td>739.3</td>\n",
              "      <td>0.1703</td>\n",
              "      <td>0.5401</td>\n",
              "      <td>0.5390</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.4378</td>\n",
              "      <td>0.10720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>M</td>\n",
              "      <td>12.46</td>\n",
              "      <td>24.04</td>\n",
              "      <td>83.97</td>\n",
              "      <td>475.9</td>\n",
              "      <td>0.11860</td>\n",
              "      <td>0.23960</td>\n",
              "      <td>0.22730</td>\n",
              "      <td>0.08543</td>\n",
              "      <td>0.2030</td>\n",
              "      <td>0.08243</td>\n",
              "      <td>0.2976</td>\n",
              "      <td>1.5990</td>\n",
              "      <td>2.039</td>\n",
              "      <td>23.94</td>\n",
              "      <td>0.007149</td>\n",
              "      <td>0.07217</td>\n",
              "      <td>0.07743</td>\n",
              "      <td>0.01432</td>\n",
              "      <td>0.01789</td>\n",
              "      <td>0.010080</td>\n",
              "      <td>15.09</td>\n",
              "      <td>40.68</td>\n",
              "      <td>97.65</td>\n",
              "      <td>711.4</td>\n",
              "      <td>0.1853</td>\n",
              "      <td>1.0580</td>\n",
              "      <td>1.1050</td>\n",
              "      <td>0.2210</td>\n",
              "      <td>0.4366</td>\n",
              "      <td>0.20750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   y     x1     x2      x3      x4  ...     x26     x27     x28     x29      x30\n",
              "0  M  17.99  10.38  122.80  1001.0  ...  0.6656  0.7119  0.2654  0.4601  0.11890\n",
              "1  M  20.57  17.77  132.90  1326.0  ...  0.1866  0.2416  0.1860  0.2750  0.08902\n",
              "2  M  19.69  21.25  130.00  1203.0  ...  0.4245  0.4504  0.2430  0.3613  0.08758\n",
              "3  M  11.42  20.38   77.58   386.1  ...  0.8663  0.6869  0.2575  0.6638  0.17300\n",
              "4  M  20.29  14.34  135.10  1297.0  ...  0.2050  0.4000  0.1625  0.2364  0.07678\n",
              "5  M  12.45  15.70   82.57   477.1  ...  0.5249  0.5355  0.1741  0.3985  0.12440\n",
              "6  M  18.25  19.98  119.60  1040.0  ...  0.2576  0.3784  0.1932  0.3063  0.08368\n",
              "7  M  13.71  20.83   90.20   577.9  ...  0.3682  0.2678  0.1556  0.3196  0.11510\n",
              "8  M  13.00  21.82   87.50   519.8  ...  0.5401  0.5390  0.2060  0.4378  0.10720\n",
              "9  M  12.46  24.04   83.97   475.9  ...  1.0580  1.1050  0.2210  0.4366  0.20750\n",
              "\n",
              "[10 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBGSdyGIbhwv",
        "outputId": "b2a437bc-501d-485e-b016-6cb926025570",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_fory=LabelEncoder()\n",
        "data.iloc[:,0]=labelencoder_fory.fit_transform(data.iloc[:,0].values)\n",
        "data.dtypes"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "y        int64\n",
              "x1     float64\n",
              "x2     float64\n",
              "x3     float64\n",
              "x4     float64\n",
              "x5     float64\n",
              "x6     float64\n",
              "x7     float64\n",
              "x8     float64\n",
              "x9     float64\n",
              "x10    float64\n",
              "x11    float64\n",
              "x12    float64\n",
              "x13    float64\n",
              "x14    float64\n",
              "x15    float64\n",
              "x16    float64\n",
              "x17    float64\n",
              "x18    float64\n",
              "x19    float64\n",
              "x20    float64\n",
              "x21    float64\n",
              "x22    float64\n",
              "x23    float64\n",
              "x24    float64\n",
              "x25    float64\n",
              "x26    float64\n",
              "x27    float64\n",
              "x28    float64\n",
              "x29    float64\n",
              "x30    float64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGQIsM_jc-08"
      },
      "source": [
        "X=data.iloc[:, data.columns != 'y'].values\n",
        "Y=data['y'].values\n",
        "# type(Y)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfyQpEInd396"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbeHTnQyBjFb"
      },
      "source": [
        "#### Step 3: Data Partitioning <font color='blue'>(5 Points)</font>\n",
        "\n",
        "1. Partition your data into training (80%), validation (20%) and testing data(20%) using sklearn library (Hint: use train_test_split)\n",
        "2. Seperate Target Label (y) and Features (x1 to x30) for training, validation and testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm8hCEB7BjFc",
        "outputId": "68a1a496-4e03-47c7-c829-5af3b53892b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#splitting train-80%,val-10%,test-10%,for second train_test split 10% will be corresponds to 11.11111...%\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=42) #step 3\n",
        "# X_train.shape,X_test.shape\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=1/9, random_state=42)\n",
        "X_train.shape,X_val.shape,X_test.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((400, 30), (50, 30), (50, 30))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWJ5cSu5BjFg"
      },
      "source": [
        "#### Step 4: Scaling Features <font color='blue'>(5 Points)</font>\n",
        "\n",
        "One simple scaling function that you could use off the shelf is Min Max Scaler function of Sklearns. Min Max scaler function transforms features by scaling each feature to a range between <b> 0 and 1 </b>. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n",
        "\n",
        "The transformation is given by:\n",
        "\n",
        "$X_{std} = \\frac{X - X_{min}}{X_{max} - X_{min}}$\n",
        "\n",
        "$X_{scaled} = X_{std} * (maxRange - minRange) + minRange$\n",
        "\n",
        "$maxRange$ = 1, <br>\n",
        "$minRange$ = 0, <br>\n",
        "$X_{max}$ and $X_{min}$ are over axis = 0 (Each columns max and min value) \n",
        "\n",
        "##### Why do we need to scale features?\n",
        "We scale the data to bring all the features to the same range (in our case between 0 and 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdirt44JBjFh"
      },
      "source": [
        "#### Hint, make sure the dimensionality of the training features, weights, biases are consistent for np.dot and np.multiply"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFryF8NXBjFi"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaler_train = scaler.fit_transform(X_train)\n",
        "X_scaler_test = scaler.transform(X_test)\n",
        "X_scaler_val=scaler.transform(X_val)\n",
        "# X_scaler_test,X_scaler_train,X_scaler_val"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7aQgKLQUuPI"
      },
      "source": [
        "# X_scaler_train.shape,X_scaler_train.shape,X_scaler_train.shape,X_scaler_train.shape"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvNixGQRM-Mn"
      },
      "source": [
        "# Step 4 already implemented\n",
        "# print(Y_train.shape[0],Y_train.shape,Y_train,X_scaler_train)\n",
        "y_train_arr = Y_train.reshape(Y_train.shape[0],1).T\n",
        "x_train_arr = X_scaler_train.T\n",
        "y_val_arr = Y_val.reshape(Y_val.shape[0],1).T\n",
        "x_val_arr = X_scaler_val.T\n",
        "y_test_arr  = Y_test.reshape(Y_test.shape[0],1).T\n",
        "x_test_arr  = X_scaler_test.T"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir_WKpy672-8"
      },
      "source": [
        "def prediction(X, w, b):\n",
        "    y_predict = np.zeros((1,X.shape[1] ))\n",
        "    Z=np.dot(w.T, X)+b\n",
        "    A_pred=1/(1+np.exp(-(Z)))\n",
        "    for i in range(A_pred.shape[1]):\n",
        "        if(A_pred[0,i]>0.5):y_predict[0,i]=1\n",
        "        else:y_predict[0,i]=0\n",
        "    return y_predict"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvMupdggBjFm"
      },
      "source": [
        "#### Step 5: Initialization of Variables\n",
        "* Initialize Hyper-Parameters (Learning Rate, Number of Epochs) to some value\n",
        "* Initialize weights to any random values (We have initialized weights as an array random values sampled from a normal distribution)\n",
        "* Initialize bias to any scalar value (We have initialized bias to value 0)\n",
        "* Initialize other variables which may be used for tracking cost, number of data points etc.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIAIAyCTBjFn"
      },
      "source": [
        "# Hyper parameter initialization\n",
        "learningrate = 0.01\n",
        "epochs = 30000\n",
        "bias = 0\n",
        "\n",
        "# intialize matrix with random values for weights with normal distribution\n",
        "# Return a sample (or samples) from the “standard normal” distribution.\n",
        "weights = np.random.randn(30,1)\n",
        "training_cost = []\n",
        "val_cost = []\n",
        "training_accuracy=[]\n",
        "validation_accuracy=[]\n",
        "\n",
        "w=weights\n",
        "b=bias\n",
        "  "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkaBAQ4RBjFr"
      },
      "source": [
        "#### Step 6: TRAINING with Logistic Regression Implementation using Gradient Descent Algorithm <font color='blue'>(30 Points)</font>\n",
        "\n",
        "Iteratively update the weights and biases for each epoch using:\n",
        "* Step 6.1: Use genesis equation $\\hat{y} = \\sigma (W.X + b)$ where $W$ is the weight array, $X$ is the input features and $\\hat{y}$ is the predicted value which will be between 0 and 1. (You will have to perform same operation on validation set as well)\n",
        "* Step 6.2: Find Binary Cross Entropy Cost for training and validation set using predicted value $\\hat{y}$ and truth value $y$\n",
        "* Step 6.3: Find $ \\Delta W = \\frac{\\delta L}{\\delta W}$ and $ \\Delta b = \\frac{\\delta L}{\\delta b}$ (Proof for finding  $ \\Delta W$ and $\\Delta b$ is available in prof. slides)\n",
        "* Step 6.4: Update $W$ and $b$ using learning rate as follows:\n",
        "  - $W = W - learningRate*\\Delta W$\n",
        "  - $b = b - learningRate*\\Delta b$\n",
        "* Step 6.5: Store BCE Cost for training and validation in seperate cost tracking list\n",
        "* Step 6.6: Calculate Training and Validation Accuracy and store in seperate accuracy tracking list (<b>Hint</b>: Threshold $\\hat{y}$ to 0.5 for category determination for finding accuracy)\n",
        "\n",
        "Run step 6 multiple times, each time with a different set of hyperparameters and determine the best set of hyperparameters which gives best training and validation accuracy. \n",
        "\n",
        "Corresponding to the best set of hyper parameters, you should get the best set of weights and bias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69vmfvqGNpcL",
        "outputId": "aa034018-ecde-41a1-8e60-9d5f706a5947",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def accuracy_value(predictions , truth):\n",
        "  return (100 - np.mean(np.abs(predictions - truth)) * 100)\n",
        "\n",
        "for i in range(epochs):\n",
        "    m_train = x_train_arr.shape[1]\n",
        "    Z_train = np.dot(w.T, x_train_arr) + b    \n",
        "    \n",
        "    A_train=1/(1+np.exp(-Z_train))\n",
        "    cost_train= -(1/m_train) * np.sum(y_train_arr * np.log(A_train) + (1-y_train_arr) * np.log(1-A_train))\n",
        "\n",
        "    m_val = x_val_arr.shape[1]\n",
        "    Z_val = np.dot(w.T, x_val_arr) + b    \n",
        "    \n",
        "    A_val=1/(1+np.exp(-Z_val))\n",
        "    cost_val= -(1/m_val) * np.sum(y_val_arr * np.log(A_val) + (1-y_val_arr) * np.log(1-A_val))\n",
        "\n",
        "    dw = (1/m_train)* np.dot(x_train_arr, (A_train-y_train_arr).T)\n",
        "    db = (1/m_train)* np.sum(A_train-y_train_arr)\n",
        "    w = w - learningrate * dw\n",
        "    b = b - learningrate * db\n",
        "\n",
        "    training_cost.append(cost_train)\n",
        "    val_cost.append(cost_val)\n",
        "    Y_prediction_train = prediction(x_train_arr, w, b)\n",
        "    Y_prediction_val = prediction(x_val_arr, w, b)\n",
        "    training_accuracy.append(accuracy_value(Y_prediction_train , y_train_arr))\n",
        "    validation_accuracy.append(accuracy_value(Y_prediction_val , y_val_arr))\n",
        "\n",
        "    \n",
        "#calculating accuracy \n",
        "Y_prediction_train = prediction(x_train_arr, w, b)\n",
        "Y_prediction_val = prediction(x_val_arr, w, b)\n",
        "Y_prediction_test = prediction(x_test_arr,w, b)\n",
        "      \n",
        "print(\"\\n Training accuracy is : {} %\".format(accuracy_value(Y_prediction_train , y_train_arr)))\n",
        "print(\"\\n validation accuracy is : {} %\".format(accuracy_value(Y_prediction_val , y_val_arr)))   \n",
        "print(\"\\n Test accuracy is : {} %\".format(accuracy_value(Y_prediction_test , y_test_arr)))\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Training accuracy is : 96.0 %\n",
            "\n",
            " validation accuracy is : 92.0 %\n",
            "\n",
            " Test accuracy is : 92.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-ZcnoTOBjFv"
      },
      "source": [
        "After performing all the steps above, the best set of updated weights and bias should be stored as a <i>weights_biases.csv</i> file. Your <i>weights_biases.csv</i> will tested on a hidden test set and you would be graded on how well your model (weights) performed on this hidden set <font color='blue'>(30 Points)</font>\n",
        "\n",
        "<b>(Do not change the code provided in the cell below for storing the weights and bias)</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg8pxLjcBjFw",
        "outputId": "acd38f2e-099f-43f9-cfeb-137aa9b9fbce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Save the weights file (DO NOT CHANGE THIS CODE)\n",
        "weights_bias = np.append(weights,bias)\n",
        "\n",
        "if weights_bias.shape == (31,):\n",
        "    print(\"Weights and Bias consistent :) \")\n",
        "    savetxt('weights_bias.csv', weights_bias, delimiter=',')\n",
        "else:\n",
        "    print(\"Weights and Bias inconsistent :( \\\\\n",
        "          Weights array shape should be (30,) and Bias should be a scalar \\\\\n",
        "          weights_bias variable should be shaped as (31,)\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights and Bias consistent :) \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l_NEBUaBjFz"
      },
      "source": [
        "#### Step 7: Plot Training and Validation Cost vs Number of Epochs <font color='blue'>(5 Points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "JDi6_DhUBjF0",
        "outputId": "a7904ba9-d67a-43b2-c450-eafefd664e48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.title('Training and Validation Loss With Learning Rate: ' + str(learningrate))\n",
        "plt.plot(training_cost, color='red', label='Training Data')\n",
        "plt.plot(val_cost, color='blue', label='Validation Data')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2RFOL_7BjF2"
      },
      "source": [
        "####  Step 8: Plot Training and Validation Accuracy vs Number of Epochs <font color='blue'>(5 Points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkYXaH9uBjF3",
        "outputId": "e9ccda86-ad68-42fd-83bf-f4a40fad501d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.title('Training and Validation Accuracy With Learning Rate: ' + str(learningrate))\n",
        "plt.plot(training_accuracy, color='red', label='Training Data')\n",
        "plt.plot(validation_accuracy, color='blue', label='Validation Data')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJaGCDDiBjF6"
      },
      "source": [
        "#### Step 9: Test your model using tesing data <font color='blue'>(15 Points)</font>\n",
        "\n",
        "* Step 9.1: Use genesis equation $\\hat{y} = \\sigma (W.X_{test} + b)$ where $W$ is the weight array, $X_{test}$ is the input test features and $\\hat{y}$ is the predicted value which will be between 0 and 1.\n",
        "* Step 9.2: Threshold $\\hat{y}$ at 0.5 to find the category for each data point.\n",
        "* Step 9.3: Find accuracy, precision and recall for testing data (you can use sklearns.metrics library)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LowhLQneBjF7",
        "outputId": "0c5754e8-8066-472e-8b3d-c91fbfea6fff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Y_prediction_test = prediction(x_test_arr,w, b)\n",
        "print(\"\\nTest accuracy: {} %\".format(accuracy_value(Y_prediction_test , y_test_arr)))\n",
        "from sklearn.metrics import accuracy_score,recall_score,average_precision_score\n",
        "accuracy_score(Y_test, Y_prediction_test.flatten())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test accuracy: 92.0 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.92"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIUz-CsQT61F",
        "outputId": "61c71b30-84b7-4ed2-9940-b10fa7cd2016",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "recall_score(Y_test, Y_prediction_test.flatten())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8333333333333334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g3HVVcTU5_n",
        "outputId": "93f2ca68-e4ff-441e-92ac-ccada23a94a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "precision_score(Y_test, Y_prediction_test.flatten())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHu9NZ_QVldW"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysjjBEwLT83v"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_81gvknWBjF-"
      },
      "source": [
        "#### Step 10: Submission to timberlake server\n",
        "\n",
        "* The code for your implementation should be in this Python notebook with necessary comments within the code.\n",
        "\n",
        "* Your <b> Python Code file </b> `main.ipynb`, <b> Data File </b> `wdbc.csv` and your <b>Trained Weights and Bias File</b> `weights_bias.csv`</b> should be put in a single folder named as `proj1code`. \n",
        "\n",
        "* `proj1code` folder should be zipped with the resulting zip file name as `proj1code.zip`.\n",
        "\n",
        "* Submit the Python code on CSE timberlake server with the following script:\n",
        "\n",
        " - `submit_cse474 proj1code.zip` for undergraduates\n",
        " - `submit_cse574 proj1code.zip` for graduates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9AL05GZBjF_"
      },
      "source": [
        "### Grading Rubric\n",
        "* <b>30 Points:</b> Your trained `weights_bias.csv` will be automatically graded using a script on unbiased hidden test data file. Hence, it is important that your `weights_biases` should of dimensionality (31,) and properly trained.\n",
        "* <b>30 Points:</b> Training logic for implementing logistic regression (Step 6)\n",
        "* <b>15 Points:</b> Testing Accuracy, Precision and Recall (Step 9)\n",
        "* <b>5 Points:</b> Plot of Training and Validation cost vs No. of epochs (Step 7) \n",
        "* <b>5 Points:</b> Plot of Training and Validation accuracy vs No. of epochs (Step 8)\n",
        "* <b> 5 points: </b> Scaling features (Step 4)\n",
        "* <b> 5 points: </b> Partitioning Data (Step 3)\n",
        "* <b> 5 points: </b> Data loading (Step 2)"
      ]
    }
  ]
}